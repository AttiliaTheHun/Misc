from enum import Enum

class Token:
    """
    This class represents a markdown language token. A language token is small chunk of the original message that may have a specific role in stating what
    output should the parser produce.
    """
    class Type(Enum):
        TEXT = 0
        SPACE = 2
        TAB = 3 # Deprecated, tabs are being replaced with 4 SPACE tokens now
        EOL = 4 # EOL tokens are no longer generated by the Tokenizer
        EOF = 5 # Deprecated
        BACKSLASH = 6
        HASH = 7
        GT = 8 # greater than symbol
        LT = 9 # less than symbol
        NUMBER = 10
        EXCLAMATION_MARK = 12
        DASH = 13
        ASTERISK = 14
        UNDERSCORE = 15
        PLUS = 16
        TILDE = 17 # Deprecated, tilde does not play any special role in the Markdown markup
        EQUALS = 18
        LPAREN = 19
        RPAREN = 20
        LBRACE = 21
        RBRACE = 22
        LBRACKET = 23
        RBRACKET = 24
        DOUBLE_QUOTES = 25
        SINGLE_QUOTES = 26
        COLON = 27
        BACKTICK = 28
        PERIOD = 29
        SEMICOLON = 30
        
        def __str__(self):
            return self.name

    escapable_tokens = [Type.BACKSLASH, Type.BACKTICK, Type.ASTERISK, Type.UNDERSCORE, Type.LBRACE, Type.RBRACE,
        Type.LBRACKET, Type.RBRACKET, Type.LPAREN, Type.RPAREN, Type.HASH, Type.PLUS, Type.DASH,
        Type.PERIOD, Type.EXCLAMATION_MARK]

    def __init__(self, value, kind, line=0, position=0):
        """
        @value is the text content of the token
        @kind specifies the type of the token
        @line specifies the line number from where the token was extracted (for error feedback)
        @position specifies the position on the line the token was extracted from (for error feedback)
        """
        self.value = value
        self.kind = kind
        self.line = line # Deprecated
        self.position = position # Deprecated
    
    def __eq__(self, other):
        if not isinstance(other, Token):
            return False
        if self.value != other.value:
            return False
        if self.kind != other.kind:
            return False
        return True
    
    def __str__(self):
        return f"{{ value: '{self.value}', kind: {self.kind.name if self.kind is not None else 'None'} }}"
        
    def __repr__(self):
        return str(self)
    
    def get_indent(self):
        """
        @Deprecated
        Returns the amount of indentation in spaces this token represents.
        Returns an integer.
        """
        if self.kind is Token.Type.SPACE:
            return 1
        elif self.kind is Token.Type.TAB:
            return 4
        return 0

class Tokenizer:
    """
    This class is used to convert text into Markdown language tokens. The transition from text to language tokens is necessary for the parser.
    """
    def tokenize(self, text):
        """
        Converts a string of text into lists of tokens, where each list represents a line of the source text.
        Returns a list of lists of tokens.        
        """
        if text is None:
            raise ValueError("Can not tokenize None")
        if len(text) == 0:
            return []
        tokens = []    
        lines = text.split("\n")
        for i in range(len(lines)):
            position = self.__tokenize_line(lines[i], tokens)
            
        #tokens[-1].append(Token("", Token.Type.EOF, len(lines) + 1, 0))
        return tokens
        
        
    def __tokenize_line(self, line, target):
        """
        Goes through the string @line and converts it into tokens. These tokens will be appended as a list to @target. The @line_number indicates the
        number these tokens are on.
        """
        token_start = -1
        token_type = None
        tokens = []
        for i in range(len(line)):
            match line[i]:
                case " ":
                    if token_type is not Token.Type.SPACE:
                        token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                        token_start = i
                        token_type = Token.Type.SPACE
                case "\t":
                    tokens.append(Token(" " * 4 , Token.Type.SPACE, i))
                case "\n":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    # explicit line breaks are deprecated, tokens are now returned as a line
                    # tokens.append(Token("", Token.Type.EOL, i))
                    break
                    
                case "\\":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.BACKSLASH, i))
                case "#":
                    if token_type is not Token.Type.HASH:
                        token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                        token_start = i
                        token_type = Token.Type.HASH
                case "<":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.LT, i))
                case ">":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.GT, i))
                case "!":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.EXCLAMATION_MARK, i))
                case "*":
                    if token_type is not Token.Type.ASTERISK:
                        token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                        token_start = i
                        token_type = Token.Type.ASTERISK
                case "-":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.DASH, i))
                case "_":
                    if token_type is not Token.Type.UNDERSCORE:
                        token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                        token_start = i
                        token_type = Token.Type.UNDERSCORE
                case "+":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.PLUS, i))
                case "=":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.EQUALS, i))
                case "(":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.LPAREN, i))
                case ")":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.RPAREN, i))
                case "[":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.LBRACKET, i))
                case "]":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.RBRACKET, i))
                case "{":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.LBRACE, i))
                case "}":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.RBRACE, i))
                case ".":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.PERIOD, i))
                case "'":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.SINGLE_QUOTES, i))
                case "\"":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.DOUBLE_QUOTES, i))
                case "`":                
                    if token_type is not Token.Type.BACKTICK:
                        token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                        token_start = i
                        token_type = Token.Type.BACKTICK
                case ":":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.COLON, i))
                case ";":
                    token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                    tokens.append(Token("", Token.Type.SEMICOLON, i))
                
                case _:
                    if line[i].isdigit():
                        if token_type is not Token.Type.NUMBER:
                            token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                            token_start = i
                            token_type = Token.Type.NUMBER
                    else:
                        if token_type is not Token.Type.TEXT:
                            token_start, token_type = self.__finish_open_token(line, tokens, token_start, token_type, i)
                            token_start = i
                            token_type = Token.Type.TEXT
        self.__finish_open_token(line, tokens, token_start, token_type, len(line))
        target.append(tokens)
        
    def __finish_open_token(self, line, tokens, token_start, token_type, i):
        """
        When the iterator encounters a token that can potentially contain multiple characters of content, it has to store these characters until the token
        ends. The is buffer converted to a token as soon as another type of token is encountered. If the end of line is reached first, there will still be 
        characters left the buffer.
                        
        This function should only be used to close multi-character tokens such as text strings or integer literals, its use upon a single character tokens
        results in undefined behavior, because these tokens are handled differently.
        
        Returns a tuple of
        
                1) -1 to reset the token_start variable
                2) None to reset the token_type variable
        """
        if token_start != -1:
            tokens.append(Token(line[token_start:i], token_type))
        return (-1, None)




        

